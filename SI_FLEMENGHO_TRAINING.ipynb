{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% Step 1: Import the necessary Libraries\n"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import load_data as ld\n",
    "import shap\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import pickle\n",
    "from scipy.spatial import distance\n",
    "from piecewise import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% Step 2: Define function to determine the optimal number of clusters and separate the data based on sex\n"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_inertia(predictions, data, centroids):\n",
    "    dist = 0\n",
    "    data = data.to_numpy()\n",
    "    for k in range(len(data)):\n",
    "        centroid_index = predictions[k]\n",
    "        dist += distance.euclidean(data[k], centroids[centroid_index])\n",
    "    return dist\n",
    "\n",
    "def define_number_of_clusters(z, path):\n",
    "    inertia_score = []\n",
    "    sh_score = []\n",
    "    db_score = []\n",
    "    bic_score = []\n",
    "    ch_score = []\n",
    "    for cl in range(2, 11):\n",
    "        clustering = GaussianMixture(n_components=cl, n_init=100, random_state=0, covariance_type=\"diag\").fit(z)\n",
    "        predictions = clustering.predict(z)\n",
    "\n",
    "        bic_score.append(clustering.bic(z))\n",
    "        inertia_score.append(calculate_inertia(predictions, z, clustering.means_))\n",
    "        sh_score.append(silhouette_score(z, predictions))\n",
    "        db_score.append(davies_bouldin_score(z, predictions))\n",
    "        ch_score.append(calinski_harabasz_score(z, predictions))\n",
    "\n",
    "    inertia_score = np.array(inertia_score)\n",
    "    inertia_score = inertia_score/max(inertia_score)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(list(range(2, 11)), inertia_score, \"-*\")\n",
    "    plt.xlabel(\"Number of Clusters\")\n",
    "    plt.ylabel(\"Sum of Distances (Normalised)\")\n",
    "    plt.xticks(list(range(2, 11)), list(range(2, 11)))\n",
    "    plt.savefig(os.path.join(path, 'Define Clusters (Inertia Score).svg'))\n",
    "    plt.savefig(os.path.join(path, 'Define Clusters (Inertia Score).png'))\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(list(range(2, 11)), sh_score, \"-*\")\n",
    "    plt.xlabel(\"Number of Clusters\")\n",
    "    plt.ylabel(\"Silhouette Score\")\n",
    "    plt.xticks(list(range(2, 11)), list(range(2, 11)))\n",
    "    plt.savefig(os.path.join(path, 'Define Clusters (Silhouette).svg'))\n",
    "    plt.savefig(os.path.join(path, 'Define Clusters (Silhouette).png'))\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(list(range(2, 11)), db_score, \"-*\")\n",
    "    plt.xlabel(\"Number of Clusters\")\n",
    "    plt.ylabel(\"Davies-Bouldin Score\")\n",
    "    plt.xticks(list(range(2, 11)), list(range(2, 11)))\n",
    "    plt.savefig(os.path.join(path, 'Define Clusters (Davies-Bouldin).svg'))\n",
    "    plt.savefig(os.path.join(path, 'Define Clusters (Davies-Bouldin).png'))\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(list(range(2, 11)), bic_score, \"-*\")\n",
    "    plt.xlabel(\"Number of Clusters\")\n",
    "    plt.ylabel(\"BIC Score\")\n",
    "    plt.xticks(list(range(2, 11)), list(range(2, 11)))\n",
    "    plt.savefig(os.path.join(path, 'Define Clusters (BIC).svg'))\n",
    "    plt.savefig(os.path.join(path, 'Define Clusters (BIC).png'))\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(list(range(2, 11)), ch_score, \"-*\")\n",
    "    plt.xlabel(\"Number of Clusters\")\n",
    "    plt.ylabel(\"Calinski-Harabasz Score\")\n",
    "    plt.xticks(list(range(2, 11)), list(range(2, 11)))\n",
    "    plt.savefig(os.path.join(path, 'Define Clusters (CH).svg'))\n",
    "    plt.savefig(os.path.join(path, 'Define Clusters (CH).png'))\n",
    "    plt.show()\n",
    "\n",
    "def retrieve_strain(root_path, full_path, avc, p_waves):\n",
    "    #read the time, strain and ECG data from the .txt files\n",
    "    original_data, data, patient_id, interval = ld.read_data(full_path)\n",
    "\n",
    "    #Obtain the time AVC from the respective .xlsx file, as annotated manually be an expert\n",
    "    #IDs of patients that do not have a measurement are included in the \"excluded_patients1\" variable\n",
    "    excluded_patients1, avc_times = ld.read_avc_time(root_path, avc)\n",
    "\n",
    "    #Read the time at which the peak of the P-wave occurs, as annotated manually be an expert\n",
    "    #IDs of patients that do not have a measurement are included in the \"excluded_patients2\" variable\n",
    "    excluded_patients2, p_wave_times = ld.read_p_wave_data(root_path, p_waves)\n",
    "\n",
    "    #Remove from the time, strain and ECG data, the measurements that correspond to IDS that should be excluded\n",
    "    original_data, data, patient_id, interval = exclude_patients(excluded_patients1, excluded_patients2, original_data,\n",
    "                                                                 data, patient_id, interval)\n",
    "    return original_data, patient_id, interval, avc_times, p_wave_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the data\n",
    "parent_folder = os.path.join(*Path(os.getcwd()).parts[:-2], \"LV Strain Curves\")\n",
    "data_path_16 = os.path.join(parent_folder, \"Data/FLEMENGHO/Strain curves - Sfile16 (Filtered)\")\n",
    "#manual annotation of the Aortic Valve Closure\n",
    "avc_files = [\"Data/FLEMENGHO/AVC time_16_all.xlsx\", \"Data/FLEMENGHO/Patients Without AVC_TK.xlsx\"]\n",
    "#manual annotation of the P-wave in the ECG\n",
    "marker_file = [\"Data/FLEMENGHO/Patients for manual annotation of markers_TK.xlsx\"]\n",
    "\n",
    "LV_original_data, LV_patient_id, LV_interval, aortic_closure, p_wv = retrieve_strain(parent_folder, data_path_16, avc_files, marker_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Temporal Alignment\n",
    "decision = \"peak\"\n",
    "reference_patient_id = \"1687\"\n",
    "LV_ecg_aligned, LV_deformation, _, _, LV_reference_time = get_aligned_signals(LV_original_data, decision, LV_interval,\n",
    "                                                                              LV_patient_id,reference_patient_id, aortic_closure, p_wv)\n",
    "\n",
    "LV_deformation = np.array(LV_deformation)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Extract the desired features\n",
    "save_data_path = os.path.join(*Path(os.getcwd()).parts[:-2], f\"Results/Summary Index/GMM/LV\")\n",
    "if not os.path.exists(save_data_path):\n",
    "    os.makedirs(save_data_path)\n",
    "\n",
    "extracted_features = extract_time_series_features(LV_reference_time, LV_deformation, LV_patient_id, save_data_path, do_plot=False)\n",
    "training_data = {\"peak\": extracted_features[\"Peak\"], \"peak_slopes\": extracted_features[[\"Systolic Slope\", \"Diastolic Slope\",\"Peak\"]], \"all\":extracted_features}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Step 5: Select Dataset\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "select_features = \"all\"\n",
    "save_data_path = os.path.join(save_data_path, select_features)\n",
    "if not os.path.exists(save_data_path):\n",
    "    os.makedirs(save_data_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Search for the optimal number of clusters\n",
    "define_number_of_clusters(training_data[select_features], save_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perform clustering with 4 clusters\n",
    "clustering_model = GaussianMixture(n_components=4, n_init=30, random_state=0, covariance_type=\"diag\").fit(training_data[select_features])\n",
    "clusters = clustering_model.predict(training_data[select_features])\n",
    "centres = clustering_model.means_\n",
    "pickle.dump(clustering_model, open(os.path.join(save_data_path, \"gmm_model.pkl\"), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% Step 12: Plot the clustering results\n"
    }
   },
   "outputs": [],
   "source": [
    "## Explain the model with SHAP values\n",
    "explainer = shap.KernelExplainer(clustering_model.predict_proba, training_data[select_features])\n",
    "shap_values_bsw = explainer.shap_values(training_data[select_features])\n",
    "\n",
    "representative_centers = np.array(produce_centroids(clusters, LV_deformation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Plot the SHAP values per cluster\n",
    "cluster_labels = {0:1, 1:4, 2:2, 3:3}\n",
    "cluster_colours = {0:\"green\", 1:\"red\", 2:\"blue\", 3:\"blueviolet\"}\n",
    "\n",
    "for i in range(len(np.unique(clusters))):\n",
    "    shap.summary_plot(shap_values=shap_values_bsw[i], features=training_data[select_features], title=f\"Cluster {cluster_labels[i]}\", show=False)\n",
    "    plt.savefig(os.path.join(save_data_path, f\"Shap Cluster {cluster_labels[i]}.svg\"))\n",
    "    plt.savefig(os.path.join(save_data_path, f\"Shap Cluster {cluster_labels[i]}.png\"))\n",
    "    plt.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Visualize the clustering results\n",
    "\n",
    "#groups the patients' ids per cluster. Returns a list\n",
    "clustered_id = analyze_patient(clusters, LV_patient_id)\n",
    "\n",
    "#writes an excel file with the patient's ID and its assigned cluster label\n",
    "write2excel(clusters, LV_patient_id, save_data_path, cluster_labels)\n",
    "\n",
    "#plots the strain traces grouped per cluster with Matplotlib and plotly. Matplotlib produces png and svg files.\n",
    "visualize_clustering_results(LV_reference_time, LV_deformation, clustered_id, clusters,\n",
    "                             LV_patient_id, representative_centers, save_data_path,\n",
    "                             cluster_labels=cluster_labels, cluster_colours=cluster_colours)\n",
    "\n",
    "#plot the first three principal components of the strain curves\n",
    "plot_pca(clusters, LV_deformation, LV_patient_id, save_data_path,\n",
    "         cluster_labels=cluster_labels, cluster_colours=cluster_colours)\n",
    "\n",
    "#plots the gradient of the Strain traces with Matplotlib and plotly.\n",
    "plot_gradients(LV_deformation, LV_reference_time, clusters, clustered_id,\n",
    "               LV_patient_id, save_data_path, cluster_labels=cluster_labels,\n",
    "               cluster_colours=cluster_colours)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
